{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "900826df-18e7-41d7-9dc8-f6599477032a",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pk2gpo1plQUJ",
    "outputId": "10a6f265-9b5e-47a6-cefb-89ede3ef0e0c"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e139fb-86b9-42b1-9203-17010985a663",
     "showTitle": false,
     "title": ""
    },
    "id": "hh5WXJ2umja4"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"hive + Jupiter\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507051a1-1386-4f66-a4fb-3b956210141d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9a960e-ee14-4676-a15f-b622bc22ad12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"use default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad6781f-5f5e-4f6b-84f0-fbb3483d5136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f8b548b-1c37-476b-9d48-286ff13ed79a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"create database practica3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23a1cd9-11bd-45bf-ba2f-6bef88710075",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8798c70-27a3-4304-ba0b-fa893fafb1e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"use practica3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213d95cb-bd62-4fd3-b894-9ce4fdfa88b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE caso_real \\\n",
    "(client_id INT, servicio_id INT, nombre_servicio STRING, meses_activo INT, domainstatus STRING, paymentmethod STRING, billingcycle STRING, total DOUBLE, state STRING) \\\n",
    "ROW FORMAT DELIMITED \\\n",
    "FIELDS TERMINATED BY ',' \\\n",
    "STORED AS TEXTFILE\")\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beab0f00-f5ee-4b4b-b5b9-4673e617cea9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31dd2b0b-9337-43d8-bb70-27de0578e65d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from caso_real\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "472d08ee-a070-401a-8485-fc0fdda02c64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"load data local inpath 'dbfs:/FileStore/practica2.csv' overwrite into table caso_real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab71d5c2-42bc-46e7-8ecc-44a983d2bc1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=spark.sql(\"select * from caso_real\")\n",
    "spark.sql(\"select * from caso_real\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6683cb48-abb4-4ff7-9855-5cd3bd6d32bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**client_id**      identificador del cliente\n",
    "\n",
    "**servicio_id**      identificador del servicio\n",
    "\n",
    "**nombre_servicio**    nombre del servicio\n",
    "\n",
    "**meses_activo**     catidad de meses con el servicio\n",
    "\n",
    "**domainstatus**     estado del servicio\n",
    "\n",
    "**paymentmethod**  metodo de pago\n",
    "\n",
    "**billingcycle**  tiempos de pago\n",
    "\n",
    "**total**    costo del servicio\n",
    "\n",
    "**state**     estado/cuidad del cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5baa47e2-8bba-4d9e-8c17-374c098ecf8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# Selecciona las variables cuantitativas que deseas analizar\n",
    "variables_cuantitativas = [\"meses_activo\", \"total\"]\n",
    "\n",
    "# Configura el estilo de Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Crea subplots para los histogramas\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(variables_cuantitativas), figsize=(15, 5))\n",
    "\n",
    "# Itera sobre las variables cuantitativas y crea histogramas\n",
    "for i, var in enumerate(variables_cuantitativas):\n",
    "    sns.distplot(df.select(var).toPandas(), bins=20, kde=True, ax=axes[i])\n",
    "    axes[i].set_xlabel(var)\n",
    "    axes[i].set_ylabel(\"Frecuencia\")\n",
    "    axes[i].set_title(f\"Histograma de {var}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e89da8-a40e-4097-8c43-c182abe12449",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selecciona las variables cuantitativas que deseas analizar\n",
    "variables_cuantitativas = [\"meses_activo\", \"total\"]\n",
    "\n",
    "# Convierte el DataFrame de Spark en un DataFrame de pandas\n",
    "df_pandas = df.select(variables_cuantitativas).toPandas()\n",
    "\n",
    "# Configura el estilo de Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Crea subplots para los boxplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(variables_cuantitativas), figsize=(15, 5))\n",
    "\n",
    "# Itera sobre las variables cuantitativas y crea boxplots\n",
    "for i, var in enumerate(variables_cuantitativas):\n",
    "    sns.boxplot(x=var, data=df_pandas, ax=axes[i])\n",
    "    axes[i].set_ylabel(var)\n",
    "    axes[i].set_title(f\"Boxplot de {var}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e26dc80-61f4-4d79-a817-6456255db20e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Supongamos que tienes una lista de variables categóricas que deseas graficar\n",
    "variables_categoricas = [\"nombre_servicio\", \"domainstatus\", \"paymentmethod\", \"billingcycle\", \"state\"]\n",
    "\n",
    "# Configura el estilo de Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Itera sobre las variables categóricas y crea un gráfico de barras para cada una en celdas separadas\n",
    "for var in variables_categoricas:\n",
    "    plt.figure(figsize=(10, 6))  # Configura el tamaño del gráfico\n",
    "    sns.countplot(data=df.toPandas(), x=var, palette=\"Set3\")\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.title(f\"Gráfico de Barras para {var}\")\n",
    "    plt.xticks(rotation=90)  # Rota las etiquetas del eje x si es necesario\n",
    "    plt.show()  # Muestra el gráfico en una celda separada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "951830f2-911b-4cb9-a703-fcc3d657ef7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162681af-2313-46c1-b5ff-b03026908602",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e16276-7b13-425f-9f95-4253864db610",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Define el esquema de tu DataFrame con tipos de datos específicos\n",
    "schema = StructType([\n",
    "    StructField(\"client_id\", IntegerType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"payments\", DoubleType(), True),\n",
    "    StructField(\"type_DISPONENT\", DoubleType(), True),\n",
    "    StructField(\"type_OWNER\", DoubleType(), True),\n",
    "    StructField(\"frequency_POPLATEK MESICNE\", DoubleType(), True),\n",
    "    StructField(\"frequency_POPLATEK PO OBRATU\", DoubleType(), True),\n",
    "    StructField(\"frequency_POPLATEK TYDNE\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Carga los datos utilizando el esquema especificado\n",
    "df_minable = spark.read.csv(\"dbfs:/FileStore/tablaminable.csv\", header=True, schema=schema)\n",
    "\n",
    "# Aplicar StringIndexer para convertir la columna \"status\" en índices numéricos\n",
    "indexer = StringIndexer(inputCol=\"status\", outputCol=\"label\")\n",
    "indexed_df = indexer.fit(df_minable).transform(df_minable)\n",
    "\n",
    "# Lista de nombres de columnas de características\n",
    "feature_cols = ['client_id',\n",
    "                'amount',\n",
    "                'payments',\n",
    "                'type_DISPONENT',\n",
    "                'type_OWNER',\n",
    "                'frequency_POPLATEK MESICNE',\n",
    "                'frequency_POPLATEK PO OBRATU',\n",
    "                'frequency_POPLATEK TYDNE']\n",
    "\n",
    "# Crear un VectorAssembler que ensambla las características en una columna llamada \"feature_vector\"\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"feature_vector\")\n",
    "\n",
    "# Aplicar el VectorAssembler al DataFrame\n",
    "df_assembled = assembler.transform(indexed_df)\n",
    "\n",
    "train_data, test_data = df_assembled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Crear un RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"feature_vector\", numTrees=100, seed=42)\n",
    "\n",
    "# Crear un Pipeline para encadenar la clasificación\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# Entrenar el modelo en los datos de entrenamiento\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Realizar predicciones en los datos de prueba\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Métricas de clasificación multiclase\n",
    "multiclass_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# F1 Score\n",
    "f1 = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: \"f1\"})\n",
    "\n",
    "# Recall\n",
    "recall = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Imprimir métricas\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41fcdaf1-1454-4f5d-b27b-a798147d175c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Entremiento_de_modelo",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
